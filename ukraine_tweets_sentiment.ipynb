{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc775ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwaks/gejopandas/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-08-07 23:49:37.313158: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-07 23:49:37.316022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-07 23:49:37.316031: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Using Poland server backend.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# import stanza as st\n",
    "# import simplemma\n",
    "\n",
    "import re\n",
    "import string\n",
    "import fasttext as ft\n",
    "import translators as ts\n",
    "import itertools\n",
    "ft_model = ft.load_model(\"lid.176.bin\")\n",
    "dir_ = 'BIG_UKR/'\n",
    "name_uk = dir_+'country_heads_topic_ukraine.xlsx'\n",
    "name = dir_+'country_heads_topic_all.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a0e512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 23:49:39.778771: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-07 23:49:39.778787: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-07 23:49:39.778798: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Kwakuter): /proc/driver/nvidia/version does not exist\n",
      "2022-08-07 23:49:39.778940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoderLabse = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\")\n",
    "\n",
    "def normalization(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, 2, axis=1, keepdims=True)\n",
    "    return embeddings/norms\n",
    "\n",
    "def val_ct_dict(obj):\n",
    "    unique, counts = np.unique(obj, return_counts=True)\n",
    "    val_ct = dict(zip(unique, counts))\n",
    "    val_ct = dict(reversed(sorted(val_ct.items(), key=lambda item: item[1])))\n",
    "    return val_ct\n",
    "\n",
    "def embedd(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        embedding = tf.constant([sentence])\n",
    "    else:\n",
    "        embedding = tf.constant(sentence)\n",
    "    embedding = encoderLabse(preprocessor(embedding))[\"default\"]\n",
    "    embedding = normalization(embedding)\n",
    "    return embedding\n",
    "\n",
    "def embedd_list(list_obj):\n",
    "    embedded = np.zeros((len(list_obj), 768))\n",
    "    with tqdm(total=len(list_obj)) as pbar:\n",
    "        for i in range(len(list_obj)):\n",
    "            embedded[i] = embedd(list_obj[i])\n",
    "            pbar.update(1)\n",
    "    return embedded\n",
    "\n",
    "def val_ct_dict(obj):\n",
    "    unique, counts = np.unique(obj, return_counts=True)\n",
    "    val_ct = dict(zip(unique, counts))\n",
    "    val_ct = dict(reversed(sorted(val_ct.items(), key=lambda item: item[1])))\n",
    "    return val_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba11e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', \n",
    "           '`', '{', '|', '}', '~', 'Â»', 'Â«', 'â€œ', 'â€']\n",
    "punct_pattern = re.compile(\"[\" + re.escape(\"\".join(punctation)) + \"]\")\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4bd356",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.ExcelFile(name_uk)\n",
    "countries = file.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709684bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_langs = ['ar', 'be', 'bs', 'ca', 'cs', 'cy', 'da', 'de', 'ds', 'el', 'en',\n",
    "#        'eo', 'es', 'eu', 'fi', 'fr', 'ga', 'hr', 'hu', 'hy', 'id', 'it',\n",
    "#        'ja', 'km', 'ko', 'lb', 'ls', 'lt', 'lv', 'mk', 'mt', 'nl', 'nn',\n",
    "#        'no', 'pl', 'pt', 'ro', 'ru', 'sh', 'sk', 'sl', 'sr', 'sv', 'th',\n",
    "#        'uk', 'vi', 'zh']\n",
    "# # all_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e2f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download de_core_news_lg\n",
    "# !python -m spacy download nl_core_news_lg\n",
    "# !python -m spacy download da_core_news_lg\n",
    "# !python -m spacy download fr_core_news_lg\n",
    "# !python -m spacy download it_core_news_lg\n",
    "# !python -m spacy download pl_core_news_lg\n",
    "# !python -m spacy download es_core_news_lg\n",
    "# !python -m spacy download pt_core_news_lg\n",
    "# !python -m spacy download ru_core_news_lg\n",
    "# !python -m spacy download ro_core_news_lg\n",
    "# !python -m spacy download fi_core_news_lg\n",
    "# !python -m spacy download lt_core_news_lg\n",
    "# !python -m spacy download sv_core_news_lg\n",
    "# !python -m spacy download el_core_news_lg\n",
    "# !python -m spacy download hr_core_news_lg\n",
    "# !python -m spacy download ca_core_news_lg\n",
    "# !python -m spacy download mk_core_news_lg\n",
    "\n",
    "# !python -m spacy download uk_core_news_trf\n",
    "# ukr-models/uk_core_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec65df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_frame = pd.read_excel(open(name, 'rb'),sheet_name='PL.1')\n",
    "fix_names = lang_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e94ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_Models():\n",
    "    culture_langs = {'AT': {'de': 207, 'en': 29, 'uk': 1, 'nl': 1, 'km': 1, 'fr': 1, 'es': 1},\n",
    "                    'BE': {'nl': 157, 'fr': 154, 'en': 66, 'de': 47, 'th': 1, 'eu': 1, 'es': 1},\n",
    "                    'BG': {'en': 5, 'lt': 1},\n",
    "                    'CY': {'el': 17, 'en': 2},\n",
    "                    'CZ': {'cz': 2137},   \n",
    "                    'EE': {'en': 176, 'uk': 8, 'pl': 2, 'ru': 1, 'nl': 1, 'lv': 1, 'ja': 1, 'it': 1, 'fr': 1, 'de': 1},\n",
    "                    'ES': {'es': 391, 'en': 4, 'fr': 2, 'zh': 1},\n",
    "                    'FI': {'fi': 182, 'en': 116, 'sv': 56, 'no': 4, 'fr': 2, 'de': 2, 'es': 1},\n",
    "                    'FR': {'fr': 106, 'it': 1, 'de': 1},\n",
    "                    'GR': {'el': 20, 'en': 4, 'th': 1, 'fr': 1},\n",
    "                    'IE': {'en': 110, 'ga': 12, 'de': 4, 'sk': 1, 'nl': 1, 'ko': 1},\n",
    "                    'IT': {'it': 131, 'es': 1},\n",
    "                    'LT': {'en': 169, 'uk': 8, 'de': 5, 'ru': 2, 'nl': 2, 'lt': 2, 'be': 2, 'hy': 1, 'fr': 1, 'da': 1},\n",
    "                    'LU': {'fr': 105, 'en': 32, 'lb': 7, 'ru': 6, 'zh': 1, 'uk': 1, 'es': 1, 'de': 1},\n",
    "                    'LV': {'lv': 579, 'en': 215, 'lt': 12, 'ja': 12, 'eu': 10, 'uk': 8, 'pl': 7, 'fr': 7, 'de': 7, 'nl': 6, 'ru': 4, 'es': 4, 'sr': 2, 'sk': 2, 'pt': 2, 'id': 2, 'th': 1, 'ro': 1, 'mk': 1, 'it': 1, 'hu': 1, 'hr': 1, 'el': 1},\n",
    "                    'MT': {'en': 133, 'mt': 2, 'fi': 1, 'de': 1},\n",
    "                    'NL': {'nl': 142, 'en': 9, 'fr': 3, 'pl': 1, 'es': 1},\n",
    "                    'PL': {'pl': 2109, 'en': 87, 'de': 16, 'pt': 8, 'uk': 7, 'fr': 7, 'es': 7, 'nl': 4, 'lt': 3, 'eo': 3, 'zh': 2, 'sv': 2, 'sl': 2, 'ja': 2, 'it': 2, 'ru': 1, 'lv': 1, 'hu': 1, 'cs': 1},\n",
    "                    'SI': {'sl': 549, 'en': 91, 'pl': 24, 'sr': 20, 'hr': 15, 'de': 9, 'it': 8, 'bs': 8, 'sh': 6, 'cs': 5, 'ru': 3, 'fr': 3, 'eo': 3, 'zh': 2, 'uk': 2, 'nl': 2, 'lt': 2, 'ar': 2, 'sk': 1, 'ro': 1, 'pt': 1, 'nn': 1, 'ls': 1, 'id': 1, 'hu': 1, 'es': 1, 'cy': 1},\n",
    "                    'SK': {'en': 62, 'uk': 1, 'sk': 1, 'pl': 1, 'fr': 1, 'cs': 1, 'ca': 1},\n",
    "                    'RO': {'en': 80, 'ro': 12, 'fr': 4, 'uk': 3, 'es': 2, 'pl': 1, 'de': 1},\n",
    "                    'GB': {'en': 858, 'nl': 2, 'ja': 2, 'fr': 2, 'vi': 1, 'ru': 1, 'es': 1, 'ds': 1, 'de': 1, 'ca': 1}}\n",
    "    \n",
    "    def __init__(self, country):\n",
    "        self.culture = country[:2]\n",
    "        self.models = dict([ [cul, self.load_nlp_core(cul.lower())] for cul in self.culture_langs[self.culture].keys()])\n",
    "    \n",
    "    def get_dominant_language(self):\n",
    "        return list(self.culture_langs[self.culture[:2]].keys())[0]\n",
    "        \n",
    "    def load_nlp_core(self, lang):\n",
    "        try:\n",
    "            print(f'{lang}_core_web_lg')\n",
    "            if lang == 'en':\n",
    "                core = spacy.load('en_core_web_lg')\n",
    "            else:\n",
    "                core = spacy.load(f'{lang}_core_news_lg')\n",
    "            return core\n",
    "        except:\n",
    "            print(f'no {lang} nlp core')\n",
    "            return None\n",
    "\n",
    "    def get_nlp_core(self, lang):\n",
    "        try:\n",
    "            return self.models[lang]\n",
    "        except:\n",
    "            print(f'no {lang} nlp core')\n",
    "            return None\n",
    "        \n",
    "    def get_stopwords(self, lang):\n",
    "        try:\n",
    "            return self.get_nlp_core(lang).Defaults.stop_words\n",
    "        except:\n",
    "            print(f'no {lang} nlp stopwords')\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4c4826b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_list = [\"fear\", \"anger\", \"anticipation\", \"trust\", \"surprise\", \"sadness\", \"disgust\", \"joy\"]\n",
    "\n",
    "def get_sentiment_vec(twitt_lem):\n",
    "# for twitt_lem, text in zip(lang_frame_uk.lemm_sentences,lang_frame_uk.text):\n",
    "    sentences = twitt_lem.split('; ')\n",
    "    for utterance in sentences:\n",
    "\n",
    "        sentiment = []\n",
    "        for word in utterance.split(', '):\n",
    "\n",
    "            e1 = embedd(word)\n",
    "            score = np.matmul(e1, np.transpose(en_index_emb))\n",
    "            if max(score[0])>0.8:\n",
    "                sentiment_tag = en_sent_lex.iloc[np.argmax(score[0])]\n",
    "#                 print(word, sentiment_tag.lem, sentiment_tag.val,max(score[0]))\n",
    "                emotions = sentiment_tag.val.split(',')\n",
    "                sentiment.append(emotions)\n",
    "        sentiment = np.array([item for sublist in sentiment for item in sublist])\n",
    "        sentiment = val_ct_dict(sentiment)\n",
    "        if sentiment.get(\"positive\") and sentiment.get(\"negative\"):\n",
    "            vibe = (sentiment[\"positive\"] - sentiment[\"negative\"])/(sentiment[\"positive\"] + sentiment[\"negative\"])*100\n",
    "            del sentiment[\"positive\"]\n",
    "            del sentiment[\"negative\"]\n",
    "        elif sentiment.get(\"positive\"):\n",
    "            vibe = 1\n",
    "            del sentiment[\"positive\"]\n",
    "        elif sentiment.get(\"negative\"):\n",
    "            vibe = -1\n",
    "            del sentiment[\"negative\"]\n",
    "\n",
    "        sentiment = dict([(i,j/sum(list(sentiment.values())) ) for i, j in sentiment.items()])\n",
    "        sentiment_ret = [ sentiment.get(s) if sentiment.get(s) else 0 for s in emotions_list]\n",
    "        return sentiment_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "048c9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text,lang_model):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(emoji_pattern, '', text)\n",
    "    text = re.sub(r'\\d+\\/\\d+', '', text)\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    text = re.sub(r' $', '', text)\n",
    "    text = re.sub(r'U/+.{5}', '', text)\n",
    "    text = re.sub(r'\\-+', '', text)\n",
    "    sentences = re.split('\\.{3} |\\. |\\! ',text)\n",
    "    langs = []\n",
    "    for x in range(len(sentences)):\n",
    "        sentences[x] = re.sub(punct_pattern, '', sentences[x])\n",
    "        sentences[x] = re.sub(r'\\n', '', sentences[x])\n",
    "    sentences = [x for x in sentences if not x.isspace() or len(x)>0]\n",
    "    for utterance in sentences:\n",
    "        langs.append(ft_model.predict(utterance)[0][0][-2:] )\n",
    "\n",
    "    if np.unique(langs).shape[0] > 1:\n",
    "        #Check for utterances translated to another languages\n",
    "        e_sentences = embedd(sentences)\n",
    "        cobinations_i = list(itertools.combinations(range(len(e_sentences)), r=2))\n",
    "        utts_to_delete = []\n",
    "        for i_ in cobinations_i:\n",
    "            if all(e_sentences[i_[0]] != e_sentences[i_[1]]):\n",
    "                c_similarity = np.matmul(e_sentences[i_[0]], np.transpose(e_sentences[i_[1]] ) )\n",
    "#                 print('-',langs[i_[0]], sentences[i_[0]],\"\\n-\",langs[i_[1]],sentences[i_[1]],\"\\n\",   c_similarity,\"\\n\",\"\\n\" )\n",
    "                if langs[i_[0]] != langs[i_[1]] and c_similarity >= 0.8: #WARNING ARBITRARY TRESHOLD!\n",
    "#                     print(sentences[i_[0]],\"\\n\",sentences[i_[1]])\n",
    "                    if langs[i_[0]] != lang_model.get_dominant_language():\n",
    "                        delete_utt = sentences[i_[0]]\n",
    "                    elif langs[i_[1]] != lang_model.get_dominant_language():\n",
    "                        delete_utt = sentences[i_[1]]\n",
    "                    utts_to_delete.append(delete_utt)\n",
    "        if len(utts_to_delete) > 0:\n",
    "            [sentences.remove(x) for x in utts_to_delete]\n",
    "\n",
    "            \n",
    "    lemm_sentences = []\n",
    "    ents = []\n",
    "    pos = []\n",
    "    sentences = [{'utterance':utt,'language':lang} for utt,lang in zip(sentences,langs) if len(utt.split(\" \"))>1]\n",
    "    for utterance in sentences:\n",
    "        nlp_core = lang_model.get_nlp_core(utterance['language'])\n",
    "        stopwords = lang_model.get_stopwords(utterance['language'])\n",
    "        if nlp_core and stopwords:\n",
    "            utt = nlp_core(utterance['utterance'])\n",
    "            utt_lem = [token.lemma_ for token in utt \n",
    "                       if token.lemma_ not in stopwords and not token.lemma_.isspace()]\n",
    "        \n",
    "#             for token in utt:\n",
    "#                 print(token.text, token.lemma_, token.pos_, token.dep_)\n",
    "            ent_ = [{'ent' : \" \".join([t.lemma_ for t in nlp_core(ent.text)]), \n",
    "                          'type' : ent.label_ } for ent in utt.ents if not (len(ent.text.split(' '))>1 and ent.label_ == 'MISC')]\n",
    "            ent_ = [ f\"{e['ent']}-{e['type']}\" if ent_ else \"Nan\" for e in ent_ ]\n",
    "            ent_ = ', '.join(ent_)\n",
    "            ents.append(ent_)\n",
    "            \n",
    "            utt_lem = ', '.join(utt_lem)\n",
    "            lemm_sentences.append(utt_lem)\n",
    "                    \n",
    "            pos_ =[token.lemma_ for token in utt if token.pos_ == 'VERB']\n",
    "            pos_ = ', '.join(pos_)        \n",
    "            pos.append(pos_)        \n",
    "        else:\n",
    "            pos.append(\"Nan\")\n",
    "            ents.append(\"Nan\")\n",
    "            lemm_sentences.append(\"Nan\")\n",
    "    ents = [i if i else 'Nan' for i in ents]\n",
    "    if ents:\n",
    "        ents = '; '.join(ents)\n",
    "    if pos:\n",
    "        pos = '; '.join(pos)\n",
    "    if lemm_sentences:\n",
    "        lemm_sentences = '; '.join(lemm_sentences)\n",
    "\n",
    "    return {\"lemm_sentences\": lemm_sentences,\"ents\": ents, \"pos\":pos }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0c7d222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB.1\n",
      "en_core_web_lg\n",
      "nl_core_web_lg\n",
      "ja_core_web_lg\n",
      "no ja nlp core\n",
      "fr_core_web_lg\n",
      "vi_core_web_lg\n",
      "no vi nlp core\n",
      "ru_core_web_lg\n",
      "es_core_web_lg\n",
      "ds_core_web_lg\n",
      "no ds nlp core\n",
      "de_core_web_lg\n",
      "ca_core_web_lg\n",
      "no ca nlp core\n"
     ]
    }
   ],
   "source": [
    "for country in countries:\n",
    "    country='GB.1'\n",
    "    print(country)\n",
    "    lang_model = Language_Models(country)\n",
    "    lang_frame_uk = pd.read_excel(open(name_uk, 'rb'),sheet_name = country)\n",
    "    if lang_frame_uk.columns.shape[0] == 0:\n",
    "        print(\"no samples\")\n",
    "        continue\n",
    "    fix_names_uk = lang_frame_uk.columns\n",
    "    lang_frame_uk.set_axis(fix_names, axis=1,inplace=True)\n",
    "    lang_frame_uk.loc[0] = fix_names_uk\n",
    "    lang_frame_uk = lang_frame_uk[[\"status_id\",\"created_at\",\"text\",\"reply_to_status_id\",\"profile_expanded_url\"]]\n",
    "\n",
    "    e = lang_frame_uk[\"text\"].apply(lambda x: process(x,lang_model))\n",
    "    lemm_sentences = [i[\"lemm_sentences\"] for i in e]\n",
    "    ents = [i[\"ents\"] for i in e]\n",
    "    pos = [i[\"pos\"] for i in e]\n",
    "\n",
    "    lang_frame_uk[\"lemm_sentences\"] = lemm_sentences\n",
    "    lang_frame_uk[\"ents\"]=ents\n",
    "    lang_frame_uk[\"pos\"]=pos\n",
    "#     lang_frame_uk['sentiment'] = lang_frame_uk['lemm_sentences'].apply(get_sentiment_vec)\n",
    "    del lang_model\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "19d79d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.07142857142857142, 0.07142857142857142, 0.2...\n",
       "1    [0.2857142857142857, 0.14285714285714285, 0.14...\n",
       "2    [0.2, 0.2, 0.2, 0.06666666666666667, 0, 0.1333...\n",
       "3    [0.1875, 0.1875, 0.125, 0.125, 0.125, 0.125, 0...\n",
       "4    [0.14814814814814814, 0.07407407407407407, 0.1...\n",
       "Name: lemm_sentences, dtype: object"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_frame_uk['lemm_sentences'].apply(get_sentiment_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d5edaba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sent_lex = pd.read_csv('lang_sentiment_dicts/en_sentiments.csv')\n",
    "en_index_emb = np.genfromtxt('lang_sentiment_dicts/en_embedded.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c8f0f3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sent_lex.shape[0]==en_index_emb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "653285cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join join positive 1.0000000251041672\n",
      "Majesty majesty positive,trust 0.8323704198388557\n",
      "Prince king positive 0.8032521887140971\n",
      "visit visit positive 1.0000000037499017\n",
      "Donation donation positive 0.8847430031121892\n",
      "Centre center positive,trust 0.9141388795714376\n",
      "excellent superb positive 0.936002075166009\n",
      "response react anger,fear 0.8516560344122279\n",
      "authority authoritative positive,trust 0.8343603331184499\n",
      "local localize anticipation 0.8073445661351674\n",
      "organisation organization anticipation,joy,positive,surprise,trust 0.9949117931883841\n",
      "humanitarian humanitarian anticipation,joy,positive,surprise,trust 1.0000000093619528\n",
      "crisis crisis negative 1.0000000158635025\n",
      "ðŸ‡ºðŸ‡¦ðŸ‡·ðŸ‡´ðŸ‡¬ðŸ‡§ Joined by Her Majesty Margareta of Romania, The Prince of Wales has visited the Romexpo Donation Centre in Bucharest to see first-hand the excellent response of the Romanian authorities and international and local organisations to the humanitarian crisis in Ukraine. https://t.co/v92d50hfA7\n",
      "['fear', 'anger', 'anticipation', 'trust', 'surprise', 'sadness', 'disgust', 'joy']\n",
      "[0.07142857142857142, 0.07142857142857142, 0.21428571428571427, 0.35714285714285715, 0.14285714285714285, 0, 0, 0.14285714285714285]\n",
      "{'trust': 0.35714285714285715, 'anticipation': 0.21428571428571427, 'surprise': 0.14285714285714285, 'joy': 0.14285714285714285, 'fear': 0.07142857142857142, 'anger': 0.07142857142857142}\n",
      "81.81818181818183\n",
      "---------------------------\n",
      "people mense positive 0.9574825693487887\n",
      "having jest joy,positive,surprise 0.8234621327554115\n",
      "flee flee fear 0.9999999471565086\n",
      "home household positive 0.8313430028307647\n",
      "escape escape anticipation,fear,negative,positive 1.0000000083165217\n",
      "conflict conflict anger,fear,negative,sadness 0.9999999661670629\n",
      "hear hearsay negative 0.8350561721604265\n",
      "support aid positive 0.8272194161213129\n",
      "provide provide anticipation,joy,positive,trust 1.0000000380464806\n",
      "displace displace anger,fear,sadness 1.0000000355804846\n",
      "With more than 11 million people having had to flee their homes to escape the conflict in Ukraine, we heard fromÂ the @decappeal about the support they are providing to those who have been displaced. https://t.co/MiosJZvfqx\n",
      "['fear', 'anger', 'anticipation', 'trust', 'surprise', 'sadness', 'disgust', 'joy']\n",
      "[0.2857142857142857, 0.14285714285714285, 0.14285714285714285, 0.07142857142857142, 0.07142857142857142, 0.14285714285714285, 0, 0.14285714285714285]\n",
      "{'fear': 0.2857142857142857, 'sadness': 0.14285714285714285, 'joy': 0.14285714285714285, 'anticipation': 0.14285714285714285, 'anger': 0.14285714285714285, 'trust': 0.07142857142857142, 'surprise': 0.07142857142857142}\n",
      "33.33333333333333\n",
      "---------------------------\n",
      "conflict conflict anger,fear,negative,sadness 0.9999999661670629\n",
      "hugely tremendously positive 0.9582832232011005\n",
      "destructive destructive anger,disgust,fear,negative 1.0000000350489215\n",
      "children child anticipation,joy,positive 0.922626872185274\n",
      "start start anticipation 0.9999999564974671\n",
      "conflict conflict anger,fear,negative,sadness 0.9999999915300104\n",
      "Patron patron positive,trust 0.879844078765057\n",
      "Princess princess positive 0.9366503102936774\n",
      "discuss discussion positive 0.9161801307547728\n",
      "impact influence negative,positive 0.9164570213593966\n",
      "crisis crisis negative 1.0000000158635025\n",
      "child child anticipation,joy,positive 1.00000000076952\n",
      "â€˜In every way, conflict is hugely destructive for children's lives.â€™\n",
      "\n",
      "One month on from the start of the conflict in #Ukraine our Patron, The Princess Royal @RoyalFamily discussed the impact this crisis is having on children around the world with @real_good_dan https://t.co/M9FrBSLrvV\n",
      "['fear', 'anger', 'anticipation', 'trust', 'surprise', 'sadness', 'disgust', 'joy']\n",
      "[0.2, 0.2, 0.2, 0.06666666666666667, 0, 0.13333333333333333, 0.06666666666666667, 0.13333333333333333]\n",
      "{'fear': 0.2, 'anticipation': 0.2, 'anger': 0.2, 'sadness': 0.13333333333333333, 'joy': 0.13333333333333333, 'trust': 0.06666666666666667, 'disgust': 0.06666666666666667}\n",
      "16.666666666666664\n",
      "---------------------------\n",
      "Countess countess positive 0.8714879274898537\n",
      "join join positive 1.0000000251041672\n",
      "Women female positive 0.8351017329768695\n",
      "representative represent positive 0.8690064425532119\n",
      "impact influence negative,positive 0.9164570439114517\n",
      "conflict conflict anger,fear,negative,sadness 0.9999999661670629\n",
      "woman female positive 0.9268531463497806\n",
      "hear hearsay negative 0.8350562316026642\n",
      "humanitarian humanitarian anticipation,joy,positive,surprise,trust 1.0000000093619528\n",
      "response react anger,fear 0.8516559833985224\n",
      "good good anticipation,joy,positive,surprise,trust 0.9999999695213755\n",
      "support aid positive 0.8272194161213129\n",
      "woman female positive 0.9268531463497806\n",
      "girl maiden positive 0.8789849395324953\n",
      "conflict conflict anger,fear,negative,sadness 0.9999999661670629\n",
      "ðŸ“Today The Countess of Wessex joined UN Women representatives and NGOs at a briefing on the impact of the conflict in Ukraine on women and girls.\n",
      "\n",
      "HRH heard about the international humanitarian response and how to best support women and girls in conflict zones.\n",
      "\n",
      "ðŸ“¸ @royalfocus1 https://t.co/kWQi1kQNFK\n",
      "['fear', 'anger', 'anticipation', 'trust', 'surprise', 'sadness', 'disgust', 'joy']\n",
      "[0.1875, 0.1875, 0.125, 0.125, 0.125, 0.125, 0, 0.125]\n",
      "{'fear': 0.1875, 'anger': 0.1875, 'trust': 0.125, 'surprise': 0.125, 'sadness': 0.125, 'joy': 0.125, 'anticipation': 0.125}\n",
      "46.666666666666664\n",
      "---------------------------\n",
      "fortunate lucky joy,positive,surprise 0.9854791979839522\n",
      "people mense positive 0.9574825693487887\n",
      "like like joy,positive,trust 0.9999999193486069\n",
      "wonderful wonderful joy,positive,surprise,trust 0.9999999939871693\n",
      "church church anticipation,joy,positive,trust 1.0000000430274532\n",
      "help assist positive,trust 0.9618026041446077\n",
      "provide provide anticipation,joy,positive,trust 1.0000000380464806\n",
      "selfless unselfish positive 0.9760016186080464\n",
      "service serve negative,trust 0.8664941911710171\n",
      "challenging challenge anger,fear,negative 0.8572018705212172\n",
      "dangerous harmful anger,disgust,fear,negative,sadness 0.8762235703092953\n",
      "condition plight anticipation,disgust,fear,negative,sadness 0.8497265641793803\n",
      "meet rencontre negative 0.8994853983186427\n",
      "daughter daughter joy,positive 1.0000000380246075\n",
      "run flee fear 0.8469956272320787\n",
      "â€œWe are so fortunate to have people like the wonderful churches here, who are all over the world - all helping, all providing selfless service under the most challenging, dangerous conditions.â€\n",
      " \n",
      "ðŸ“¸ HRH meets Andriy Kopylash and his daughter Veronika, who run Alpha Ukraine. https://t.co/FUoZGYAv2n\n",
      "['fear', 'anger', 'anticipation', 'trust', 'surprise', 'sadness', 'disgust', 'joy']\n",
      "[0.14814814814814814, 0.07407407407407407, 0.1111111111111111, 0.2222222222222222, 0.07407407407407407, 0.07407407407407407, 0.07407407407407407, 0.2222222222222222]\n",
      "{'trust': 0.2222222222222222, 'joy': 0.2222222222222222, 'fear': 0.14814814814814814, 'anticipation': 0.1111111111111111, 'surprise': 0.07407407407407407, 'sadness': 0.07407407407407407, 'disgust': 0.07407407407407407, 'anger': 0.07407407407407407}\n",
      "28.57142857142857\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for twitt_lem, text in zip(lang_frame_uk.lemm_sentences,lang_frame_uk.text):\n",
    "    sentences = twitt_lem.split('; ')\n",
    "    for utterance in sentences:\n",
    "\n",
    "        sentiment = []\n",
    "        for word in utterance.split(', '):\n",
    "\n",
    "            e1 = embedd(word)\n",
    "            score = np.matmul(e1, np.transpose(en_index_emb))\n",
    "            if max(score[0])>0.8:\n",
    "                sentiment_tag = en_sent_lex.iloc[np.argmax(score[0])]\n",
    "                print(word, sentiment_tag.lem, sentiment_tag.val,max(score[0]))\n",
    "                emotions = sentiment_tag.val.split(',')\n",
    "                sentiment.append(emotions)\n",
    "        sentiment = np.array([item for sublist in sentiment for item in sublist])\n",
    "        sentiment = val_ct_dict(sentiment)\n",
    "        if sentiment.get(\"positive\") and sentiment.get(\"negative\"):\n",
    "            vibe = (sentiment[\"positive\"] - sentiment[\"negative\"])/(sentiment[\"positive\"] + sentiment[\"negative\"])*100\n",
    "            del sentiment[\"positive\"]\n",
    "            del sentiment[\"negative\"]\n",
    "        elif sentiment.get(\"positive\"):\n",
    "            vibe = 1\n",
    "            del sentiment[\"positive\"]\n",
    "        elif sentiment.get(\"negative\"):\n",
    "            vibe = -1\n",
    "            del sentiment[\"negative\"]\n",
    "\n",
    "        sentiment = dict([(i,j/sum(list(sentiment.values())) ) for i, j in sentiment.items()])\n",
    "        sentiment_ret = [ sentiment.get(s) if sentiment.get(s) else 0 for s in emotions_list]\n",
    "        return sentiment_ret\n",
    "#         print(text)\n",
    "#         print(emotions_list)\n",
    "#         print(sentiment_ret)\n",
    "#         print(sentiment)\n",
    "#         print(vibe)\n",
    "#         print(\"---------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d01284",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_frame_uk[[\"lemm_sentences\",\"ents\", \"pos\"]] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_sentences = [i[\"lemm_sentences\"] for i in e]\n",
    "ents = [i[\"ents\"] for i in e]\n",
    "pos = [i[\"pos\"] for i in e]\n",
    "\n",
    "lang_frame_uk[\"lemm_sentences\"] = lemm_sentences\n",
    "lang_frame_uk[\"ents\"]=ents\n",
    "lang_frame_uk[\"pos\"]=pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd94206",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_frame_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301213a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = embedd(\"duck\")\n",
    "e2 = embedd(\"pond\")\n",
    "np.matmul(e1, np.transpose(e2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = embedd(\"kaczka\")\n",
    "e2 = embedd(\"staw\")\n",
    "np.matmul(e1, np.transpose(e2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b3546",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = embedd(\"kaczka\")\n",
    "e2 = embedd(\"bird\")\n",
    "np.matmul(e1, np.transpose(e2) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
